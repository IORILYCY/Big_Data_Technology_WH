 <center>
     <h1>王赫-大数据工程师简历</h1>
</center>

---

## 个人信息

* 性&ensp;别：男&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;岗&ensp;位：大数据开发工程师  
* 手&ensp;机：13263228979&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;邮&ensp;箱：dqwanghe00@163.com
* 学&ensp;历：本科&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;经&ensp;验：6年

---

## 工作及教育经历

* 复星集团&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;2021.02~至今&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;智能科技共享中心
* 软通动力&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;2019.09~2021.01&emsp;&emsp;&emsp;&emsp;平安壹账通智慧风控项目组
* 北京优购文化发展有限公司&emsp;&emsp;&emsp;&emsp;&ensp;2016.03~2019.07&emsp;&emsp;&emsp;&emsp;科技部大数据组
* 大庆油田工程建设有限公司&emsp;&emsp;&emsp;&emsp;&ensp;2010.09~2016.02&emsp;&emsp;&emsp;&emsp;化建公司第三项目部
* 东北石油大学&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;2006.9~2010.7&emsp;&emsp;&emsp;&emsp;&emsp;化学工程与工艺

---

## 专业技能

* 熟练使用 Hive、Spark、Shell 进行编程，熟悉 Java、Scala 语言，熟悉 MySQL、PgSQL 的操作
* 熟练使用 Kimball 维度模型理论对数仓分层、建模，熟悉E-R建模
* 熟练使用 Kylin 进行 Cube 的构建、优化，熟练使用 REST API 控制构建任务
* 熟悉 Hadoop、Sqoop、DataX、Kafka、Flume 的基本原理
* 熟悉阿里云 DataWorks、EMR 的开发流程
* 了解数据湖、数据虚拟化系统相关概念、系统架构
* 了解基础数据结构和算法的基本原理
* 了解 HBase、ElasticSearch、Presto

---

## 项目经历

1. 复星集团 - 集团CDP平台 - 2021.07 - 至今
    * 工作内容：
      * 负责CDP数仓数据接入、分层、维度模型建模、ETL开发
      * 负责会员Id-Mapping、标签自动计算系统、神策订阅程序的设计、开发
    * 技术要点：
      * 平台依托阿里云EMR集成Hadoop、Hive、Spark等集群组件和OSS存储系统，使用DataWorks作为统一开发接口
      * 使用DataX将产业端会员及销售数据接入CDP数仓，并完成数据脱敏
      * 使用HQL和Spark SQL完成数据清洗和ETL，利用连通图特性使用Spark GraphX完成会员Id-Mapping的计算
      * 使用DataX实现标签管理系统和CDP平台间的数据交互，完成规则数据和元数据的同步
      * 使用HQL完成规则数据的整合。编写Spark程序完成标签规则的解析、计算SQL的生成，以及会员标签的计算
      * 针对复杂的标签规则，以及从隐私计算平台获取计算结果，编写Hive UDF函数
      * 设计Structured Streaming程序对神策Kafka进行过滤，实现数据的实时订阅

2. 复星集团 - BFC AI智能中台 - 2021.02 - 至今
    * 工作内容：
      * 负责中台数仓的架构、分层设计，主题域划分、维度模型建模，ETL开发，以及部分数据源的数据同步
    * 技术要点：
      * 使用TiDB作为数仓的计算和存储平台，DataX Web作为数据同步和任务调度工具
      * 针对不同数据源，使用SQL + Shell完成对店铺数据的整合，计算统一店铺ID与源系统ID的映射
      * 使用SQL + DataX脚本完成维度表和事实表的数据清洗和数据合并，以及DWS、ADS数据指标的计算

3. 平安 - 风控一体化智慧分析平台 - 2019.09 - 2021.01
    * 工作内容：
      * 负责风控数据湖债券、股票主题域的建模、ETL开发
      * 负责智慧分析数据集群架构，Kylin与CDH、Presto的集成调研
      * 负责与平安科技沟通，实现集群部署，以及与发版、调度系统的集成
      * 负责风控数据湖到智慧分析集群的跨集群数据同步
      * 负责智慧分析Kylin Cube设计，以及构建任务的开发
    * 技术要点：
      * 使用HQL及Spark SQL完成债券、股票数据的ETL处理
      * 利用Hadoop的原生工具distcp实现风控数据的跨集群增量同步
      * 通过构建Kylin Cube提高查询效率，同时也可以减少ETL工作量
      * 使用Kylin的REST API编写Shell脚本，完成STG和PRD环境Hive表的加载、删除，以及Model、Cube的创建
      * 使用Kylin的REST API编写Shell脚本，配合Linkdo调度平台，实现Cube的Segment自动构建，及对构建任务的监控
      * 使用Presto作为Kylin的下压查询引擎，应对未创建Cube的查询需求

4. 优购
    * 工作内容：
    * 技术要点：

---

## 资格证书

* 数据库系统工程师

## 其他信息

* 喜欢钻研技术 等等
* 性格开朗，为人诚恳，认真负责
